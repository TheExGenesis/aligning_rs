from time import time
from analytics import *
from network import *
from agent import *
from mediator import *
from dilemma import *
from utils import *
from data import *
start = time()
import math
from random import choice, sample
# from graph_tool.all import *
#from graph_tool import *
# import graph_tool as gt
# import graph_tool.generation as gen
from timeit import timeit
import axelrod as axl
import pandas as pd
import toolz as tz
import numpy as np  
from numpy.random import choice
from numpy.random import rand 
from pprint import pprint
from copy import deepcopy
end = time()
print(f'[DEBUG] external imports took {end-start}s')


data experimentParams(episode_n is int)
data experimentConfig(expParams, networkParams, agentParams, gameParams, mediatorParams)
data mediatorPoolParams(mediator_params_pool is [ mediatorParams ])


#Utils
def data2Dict(data) = {name:data2Dict(data.__getattribute__(name)) for name in data._fields} if hasattr(data, '_fields') else data
def describeExperiment(config):
  print(f'Running experiment with the following config:')  
  pprint(data2Dict(config))
# # spreadRec :: (vertex, (vertex, vertex)) -> [vertex,vertex,vertex]
# def spreadRec((agent,(rec,neighbor))) = [agent, rec, neighbor]
# agentsTookRec :: [vertex] -> [vertex] -> [Boolean]
def agentsTookRec(picks,recs) = [picks[i]==recs[i] for i in range(len(picks))]
# medEvalPayoff :: vertex -> int # Evaluates how much v picking a rec pays off
def medEvalPayoff(v) = 1

# useless, memory aid
def unpackConfig(config):
  (networkParams, agentParams, gameParams, mediatorParams) = config
  (N, m, c, gamma, directed) = networkParams
  (U, pair_eval_fn, attribute_type, rec_policy_pool, dilemma_policy_pool, imitationCoef) = agentParams
  (alpha) = gameParams
  (med_pair_eval_fn) = mediatorParams
  return config

def initNet(networkParams, agentParams):
  g = genNetwork(networkParams) # config -> graph
  net = populateSocialNetwork(g, agentParams)  # graph -> config -> graph w/ agents
  return net



#Rec phase functions
# medMakeRecs :: mediator -> graph -> [vertex] -> [vertex] # Mediator makes 1 recommendation for each agent
# medMakeRecs = (med,net) -> map$(medMakeRec$(med, net))
# medMakeRecs = _medMakeRecs
# agentsPickNeighbor :: [vertex] -> [vertex] # Agents pick 1 agent from their neighborhood
agentsPickNeighbor = net -> map$(agentPickNeighbor$(net))
# (this return was made for history-keeping) agentsDecideRec :: [(vertex, (vertex, vertex))] -> [{pair: (vertex, vertex), is_rec: Boolean}] # Agent and mediator picks are compared and player pairs generated
# agentsDecideRec :: [(vertex, vertex, vertex)] -> [vertex] # Agent and mediator picks are compared and player pairs generated
agentsDecideRec = agents -> map$(x->x |*> agentDecideRec$(agents))
# medEvalPayoffs :: [vertex] -> int
medEvalPayoffs = filter$(x->x) ..> map$(medEvalPayoff)
#Dilemma phase functions
# makeGames :: [(vertex,vertex)] -> game
makeGames = agents -> map$(x->x |*> makeEncounterFromPair$(agents))
  # playGames :: [(vertex,vertex)] -> game
  
def runRecPhase(net, med):
  agents = net.vp.agents
  agentIds = net.get_vertices() |> list
  neighbors = timeF('agentsPickNeighbor', ->(agentIds |> agentsPickNeighbor(net) |> list))
  if med.pair_eval_fn != 'noMed':
    recs = timeF('medMakeRecs', ->(medMakeRecs(med, net, neighbors) |> list))
    picks = timeF('agentsDecideRec', ->[agentIds, zip(neighbors, recs)] |*> zip |> agentsDecideRec(agents) |> list)
  else:
    recs = [None for i in agentIds]
    picks = neighbors
  mediatorPayoff = timeF('medEvalPayoffs', ->(agentsTookRec(picks, recs) |> medEvalPayoffs))
  return neighbors, recs, picks, mediatorPayoff

def runDilemmaPhase(agents, agentIds, picks):
  pairs = [agentIds, picks] |*> zip |> list
  games = pairs |> makeGames(agents) |> list
  dilemmaResults = playGames(games)
  dilemmaMoves = dilemmaResults |> map$(.['moves']) |> list
  return games, dilemmaMoves

def pickRandomNeighbor(net, v) = sample(net.vertex(v).out_neighbors()|>list,1)[0]

def fermi(beta, fitness_diff):
    """
    The fermi function determines the probability that the first type imitates the second.
    :param beta: intensity of selection
    :param fitness_diff: f_a - f_b
    :return: the imitation probability
    :rtype: float
    """
    return np.clip(1. / (1. + np.exp(beta * fitness_diff, dtype=np.float64)), 0., 1.)




# decideImitate :: agents -> float -> v -> v -> dilemma_policy # who will v imitate?
def decideImitate(imitationCoef, totalScore, neighbors, v):
  def imitateProb(A,B) = fermi(imitationCoef, B-A) # beta [0,1]:: influence of diff in imitation
  # evoNeighbors :: neighbors + pick (which might be a rec)
  vScore = totalScore[v]
  neigh = sample(neighbors|>list,1)[0]
  neighScore = totalScore[neigh|>int]
  # imitate strategy with probability given by Fermi equation
  p = imitateProb(vScore, neighScore)
  return choice([v, neigh], p=[1-p,p])

# runEvolution :: (net, happenings, imitationCoef) -> net
def runEvolution(net, hap, imitationCoef):
  def evoNeighbors(v) = net.vertex(v).out_neighbors() :: (net.vertex(hap.iloc[v].pick),) |> set |> list
  agents = net.vp.agents
  newAgents = agents
  agentIds = net.get_vertices() |> list
  _decideImitate = decideImitate$(imitationCoef, hap.totalScore)
  def getNewPolicy(v) = _decideImitate(evoNeighbors(v), v) |> agents[] |> .dilemma_policy
  for v in agentIds:
  #   imitated = _decideImitate(evoNeighbors(v), v)
    newAgents[v] = getNewPolicy(v) |> changeDilemmaPolicy$(agents[v], ?)
  return newAgents

# decideImitate :: agents -> float -> v -> v -> dilemma_policy # sample score of random neighbor
# def decideImitate(imitationCoef, net, totalScore, v, adv):
#   def imitateProb(A,B) = fermi(imitationCoef, B-A) # beta [0,1]:: influence of diff in imitation
#   # evoNeighbors :: neighbors + pick (which might be a rec)
#   def evoNeighbors(v) = net.vertex(v).out_neighbors() :: (g.vertex(adv),) |> set |> list
#   agents = net.vp.agents
#   vScore = totalScore[v]
#   # neigh = pickRandomNeighbor(net,v)
#   neigh = sample(evoNeighbors(v).out_neighbors()|>list,1)[0]
#   neighScore = totalScore[neigh|>int]
#   # imitate strategy with probability given by Fermi equation
#   p = imitateProb(vScore, neighScore)
#   return choice([agents[v].dilemma_policy, agents[neigh].dilemma_policy], p=[1-p,p])

# # runEvolution :: (net, happenings, imitationCoef) -> net
# def runEvolution(net, haps, imitationCoef):
#   newNet = net
#   agents = net.vp.agents
#   agentIds = net.get_vertices() |> list
#   _decideImitate = decideImitate$(imitationCoef, net, haps.totalScore, imitationCoef)
#   for v in agentIds:
    
#     newNet.vp.agents[v] = changeDilemmaPolicy(agents[v], _decideImitate(haps.iloc[v].pick, v))
#   return newNet

def updateNetVP(name, net, vp):
  net.vp[name] = vp
  return net

# runEpisode :: net, med -> ep  #ep :: {net, happenings} # happenings :: {agents, picks, neighbors, recs, games, dilemmaMoves}
def runEpisode(net, med, imitationCoef):
  # Rec
  agents = net.vp.agents
  neighbors, recs, picks, mediatorPayoff = timeF('runRecPhase', ->runRecPhase(net, med))
  # Dilemma
  agentIds = net.get_vertices() |> list
  games, dilemmaMoves = timeF('runDilemmaPhase', ->runDilemmaPhase(agents, agentIds, picks))
  happenings = timeF('makeEpisodeLog', ->makeEpisodeLog(agentIds, agents, picks, neighbors, recs, games, dilemmaMoves) |>  x -> pd.DataFrame.from_dict(x).T)
  happeningsWPayoff = timeF('calcAllPayoffs', ->calcAllPayoffs(happenings))
  # Evo
  newAgents = timeF('runEvolution', ->runEvolution(net, happeningsWPayoff, imitationCoef))
  newNet = updateNetVP('agents', net, newAgents)
  ep = {'net':newNet , 'happenings':happeningsWPayoff} 
  # ep['happenings'] = pd.DataFrame.from_dict(ep['happenings']).T
  ep['happenings'] = pd.DataFrame.from_dict(ep['happenings'])
  return ep

def runAllEpisodes(net, med, imitationCoef, remaining_eps):
  print(f'in runAllEpisodes, {remaining_eps} remaining')
  ep = timeF(f'RUNEPISODE {remaining_eps} EPS REMAINING', ->runEpisode(net, med, imitationCoef))
  print(dilemmaPolShares(ep['net'].vp.agents))
  yield ep
  if remaining_eps <= 1: 
    print(f'returning runAllEpisodes {remaining_eps}')
    return
  else:
    print(f"recalling runAllEpisodes {remaining_eps}")
    yield from runAllEpisodes(ep['net'], med, imitationCoef, remaining_eps-1)

def runMedExperiment(imitationCoef, episode_n, net, med):  
  print(f'dilemma policy distribution: {dilemmaPolShares(net.vp.agents)}')
  return runAllEpisodes(net, med, imitationCoef, episode_n)
  # for i in range(episode_n):
  #   ep =  runEpisode(net,med)
  #   yield ep

def makeMvpConfig():
  expParams = experimentParams(episode_n = 5)
  mvpNetwork = baNetworkParams(N = 1000, m = 1)
  mvpAgent = simplestAgentParams(dilemma_policy_pool = ['Cooperator', 'Defector', 'Random'])
  mvpGame = pdGameParams(alpha = 2)
  mvpMeds = mediatorParams(pol_pool = ['noMed', 'selflessRecEval', 'naiveSelfishRecEval'])
  return experimentConfig(expParams = expParams, networkParams = mvpNetwork, agentParams = mvpAgent, gameParams = mvpGame, mediatorParams = mvpMeds)

def runExperiment(config, expName):
  describeExperiment(config)
  (expParams, networkParams, agentParams, gameParams, mediatorParams) = config
  imitationCoef = agentParams.imitationCoef
  episode_n = expParams.episode_n
  medPolPool = mediatorParams.pol_pool
  net = initNet(networkParams, agentParams) # need to store it to get a fresh copy each time(?)
  # Config
  for medPol in medPolPool:  
    episodes = runMedExperiment(imitationCoef, episode_n, deepcopy(net), mediator(medPol))
    for i,ep in enumerate(episodes):
      print(f'will store ep {i}')
      timeF(f'storeEpisode{i}', ->storeEpisode(storagePath, expName, medPol, i, ep))
      
storagePath = "../data/dev"
def main(makeConfig):
  expName = 'd'
  runExperiment(makeConfig(), expName)
  timeF(f'runAnalysis', ->runAnalysis(storagePath, expName))

timeF('main', ->main(makeMvpConfig))

# config, net, med, happenings  = consoleSetup()

DEBUG = True
#"""parameters for the model"""


# if __name__ == "__main__":
#     main()

